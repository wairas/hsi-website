{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Hyperspectral Imaging Group at the University of Waikato. We are a team of engineers and computer scientists wokring on computational imaging methods for hyperspectral imaging (HSI) technology through open-source platforms. We prioritise software-hardware integrated methods with machine learning and AI-informed image acquisition protocols. Our website serves as a hub for our software, hardware and hyperspectral database. HAPPy Software # We have developed an in-house HAPPy software, which is abbreviation for Hyperspectral Applications Platform in Python. This is a platform that streamlines the workflow for hyperspectral data acquisition, annotation, analysis, and the exploration of machine learning algorithms. HAPPy is intended to allow non-experts to interface easily with our specialised toolboxes, models and algorithms to facilitate quicker experimentation and innovation in HSI. Our Hardware # We have 2 hyperspectral cameras alongside a custom multi-instrument laboratory system for these cameras, and portable in-field imaging. Our hardware includes the Specim IQ camera (400-1000nm), the Specim FX-17 camera (900-1700nm), a rotary stage, and multiple halogen lighitng systems to capture high-quality hyperspectral data. Publications # Explore our publications to access the detailed findings and contributions from our team.","title":"Home"},{"location":"#happy-software","text":"We have developed an in-house HAPPy software, which is abbreviation for Hyperspectral Applications Platform in Python. This is a platform that streamlines the workflow for hyperspectral data acquisition, annotation, analysis, and the exploration of machine learning algorithms. HAPPy is intended to allow non-experts to interface easily with our specialised toolboxes, models and algorithms to facilitate quicker experimentation and innovation in HSI.","title":"HAPPy Software"},{"location":"#our-hardware","text":"We have 2 hyperspectral cameras alongside a custom multi-instrument laboratory system for these cameras, and portable in-field imaging. Our hardware includes the Specim IQ camera (400-1000nm), the Specim FX-17 camera (900-1700nm), a rotary stage, and multiple halogen lighitng systems to capture high-quality hyperspectral data.","title":"Our Hardware"},{"location":"#publications","text":"Explore our publications to access the detailed findings and contributions from our team.","title":"Publications"},{"location":"hardware/","text":"The hyperspectral imaging hardware available: Specim FX17 camera operates in the near-infrared region, with free wavelength selection from 224 bands within the camera coverage. Specim RS10 rotary stage scanner allows scanning an image of a stationary target or scenery in the lab and field. Manuals # Specim FX17 User Manual Specim RS10 User Guide LUMO Scanner Operational procedures Setup #","title":"Hardware"},{"location":"hardware/#manuals","text":"Specim FX17 User Manual Specim RS10 User Guide LUMO Scanner Operational procedures","title":"Manuals"},{"location":"hardware/#setup","text":"","title":"Setup"},{"location":"publications/","text":"2023 # Holmes, W.S., Ooi, M.P.L., Abeysekera, S., Kuang, Y.C., Simpkin, R., Caddie, M., Nowak, J. and Demidenko, S., 2023, May. On machine learning methods to estimate cannabidiolic acid content of Cannabis sativa L. from near-infrared hyperspectral imaging. In 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 01-06). IEEE. 10.1109/I2MTC53148.2023.10175994 Abeysekera, S.K., Robinson, A., Ooi, M.P.L., Kuang, Y.C., Manley-Harris, M., Holmes, W., Hirst, E., Nowak, J., Caddie, M., Steinhorn, G. and Demidenko, S., 2023. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. Industrial Crops and Products, 192, p.116137. 10.1016/j.indcrop.2022.116137 2022 # Ooi, M.P.L., Robinson, A., Manley-Harris, M., Hill, S., Raymond, L., Kuang, Y.C., Steinhorn, G., Caddie, M., Nowak, J., Holmes, W. and Demidenko, S., 2022. Robust statistical analysis to predict and estimate the concentration of the cannabidiolic acid in Cannabis sativa L.: A comparative study. Industrial Crops and Products, 189, p.115744. 10.1016/j.indcrop.2022.115744 2020 # Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Signal-to-noise ratio contributors and effects in proximal near-infrared spectral reflectance measurement on plant leaves. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129359 Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Lopez-Ubiria, I., Vidiella, A., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Classifying Cannabis sativa flowers, stems and leaves using statistical machine learning with near-infrared hyperspectral reflectance imaging. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129531","title":"Publications"},{"location":"publications/#2023","text":"Holmes, W.S., Ooi, M.P.L., Abeysekera, S., Kuang, Y.C., Simpkin, R., Caddie, M., Nowak, J. and Demidenko, S., 2023, May. On machine learning methods to estimate cannabidiolic acid content of Cannabis sativa L. from near-infrared hyperspectral imaging. In 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 01-06). IEEE. 10.1109/I2MTC53148.2023.10175994 Abeysekera, S.K., Robinson, A., Ooi, M.P.L., Kuang, Y.C., Manley-Harris, M., Holmes, W., Hirst, E., Nowak, J., Caddie, M., Steinhorn, G. and Demidenko, S., 2023. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. Industrial Crops and Products, 192, p.116137. 10.1016/j.indcrop.2022.116137","title":"2023"},{"location":"publications/#2022","text":"Ooi, M.P.L., Robinson, A., Manley-Harris, M., Hill, S., Raymond, L., Kuang, Y.C., Steinhorn, G., Caddie, M., Nowak, J., Holmes, W. and Demidenko, S., 2022. Robust statistical analysis to predict and estimate the concentration of the cannabidiolic acid in Cannabis sativa L.: A comparative study. Industrial Crops and Products, 189, p.115744. 10.1016/j.indcrop.2022.115744","title":"2022"},{"location":"publications/#2020","text":"Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Signal-to-noise ratio contributors and effects in proximal near-infrared spectral reflectance measurement on plant leaves. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129359 Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Lopez-Ubiria, I., Vidiella, A., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Classifying Cannabis sativa flowers, stems and leaves using statistical machine learning with near-infrared hyperspectral reflectance imaging. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129531","title":"2020"},{"location":"team/","text":"The Hyperspectral imaging team: Associate Professor Melanie Ooi Senior Lecturer Ye Chow Kuang Professor Geoffrey Holmes Dr Sanush Abeysekera Mr Dale Fletcher Mr Peter Reutemann","title":"Team"},{"location":"happy/","text":"Overview # HAPPy, short for \"Hyperspectral Application Platform in Python,\" stands as a pioneering platform at the forefront of hyperspectral imaging technology. Designed by the Hyperspectral Imaging Group at the University of Waikato, HAPPy unifies hyperspectral workflows for how hyperspectral images are acquired, processed, and analyzed. At its core, HAPPy seamlessly integrates data acquisition, annotation, analysis, and the exploration of novel statistical machine learning and deep learning approaches. Key Features # Containerized Algorithm Management: HAPPy simplifies algorithm deployment, ensuring compatibility with open-source machine learning platforms. Hardware-Software Calibration Protocols: Real-time machine learning model assessment at the imaging front enhances adaptability. Efficiency in Hyperspectral Workflow: Accelerated experimentation allows quick feasibility assessment and method shortlisting for development. Empowering Innovation: HAPPy empowers researchers to bridge the gap between hyperspectral data and state-of-the-art machine learning algorithms. It is the tool that fosters quicker experimentation, innovation, and discovery within the dynamic field of hyperspectral imaging. Open Source and Collaborative: Committed to the principles of open-source collaboration, we invite researchers, engineers, and enthusiasts from around the world to explore, contribute, and leverage the HAPPy. Get Started: Whether you're a seasoned hyperspectral imaging expert or just embarking on your journey, HAPPy is here to simplify, accelerate, and amplify your research efforts. Scroll down for information on installation and usage, as well as access to our tools and publications. Installation # The tools have been tested on Linux (Ubuntu/Debian) and under Windows with WSL2 using Ubuntu 22.04.x. Notes on WSL2 Installing the tools Usage # Once installed using the above instructions, you can launch graphical tools and Docker images (start/stop) using the happy-launch.sh script from your home directory: ./happy-launch.sh Tools # More details on the tools that are available through the HAPPy project can be found on the respective tool pages: ADAMS happy-tools Segment Anything Segment Anything in High Quality NB: These pages also contain detailed instructions on how to install them, which you can ignore if you used the happy-setup.sh installation script. Docker # For available Docker images, please see the Docker page.","title":"Introduction"},{"location":"happy/#overview","text":"HAPPy, short for \"Hyperspectral Application Platform in Python,\" stands as a pioneering platform at the forefront of hyperspectral imaging technology. Designed by the Hyperspectral Imaging Group at the University of Waikato, HAPPy unifies hyperspectral workflows for how hyperspectral images are acquired, processed, and analyzed. At its core, HAPPy seamlessly integrates data acquisition, annotation, analysis, and the exploration of novel statistical machine learning and deep learning approaches.","title":"Overview"},{"location":"happy/#key-features","text":"Containerized Algorithm Management: HAPPy simplifies algorithm deployment, ensuring compatibility with open-source machine learning platforms. Hardware-Software Calibration Protocols: Real-time machine learning model assessment at the imaging front enhances adaptability. Efficiency in Hyperspectral Workflow: Accelerated experimentation allows quick feasibility assessment and method shortlisting for development. Empowering Innovation: HAPPy empowers researchers to bridge the gap between hyperspectral data and state-of-the-art machine learning algorithms. It is the tool that fosters quicker experimentation, innovation, and discovery within the dynamic field of hyperspectral imaging. Open Source and Collaborative: Committed to the principles of open-source collaboration, we invite researchers, engineers, and enthusiasts from around the world to explore, contribute, and leverage the HAPPy. Get Started: Whether you're a seasoned hyperspectral imaging expert or just embarking on your journey, HAPPy is here to simplify, accelerate, and amplify your research efforts. Scroll down for information on installation and usage, as well as access to our tools and publications.","title":"Key Features"},{"location":"happy/#installation","text":"The tools have been tested on Linux (Ubuntu/Debian) and under Windows with WSL2 using Ubuntu 22.04.x. Notes on WSL2 Installing the tools","title":"Installation"},{"location":"happy/#usage","text":"Once installed using the above instructions, you can launch graphical tools and Docker images (start/stop) using the happy-launch.sh script from your home directory: ./happy-launch.sh","title":"Usage"},{"location":"happy/#tools","text":"More details on the tools that are available through the HAPPy project can be found on the respective tool pages: ADAMS happy-tools Segment Anything Segment Anything in High Quality NB: These pages also contain detailed instructions on how to install them, which you can ignore if you used the happy-setup.sh installation script.","title":"Tools"},{"location":"happy/#docker","text":"For available Docker images, please see the Docker page.","title":"Docker"},{"location":"happy/adams/","text":"ADAMS-based framework that can be used for annotating images using its powerful workflow engine. Requirements # OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk Installation # Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams Starting the application # Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh Available flows # Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool) Tutorials # Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation Preview browser # The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the happy-envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"ADAMS"},{"location":"happy/adams/#requirements","text":"OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk","title":"Requirements"},{"location":"happy/adams/#installation","text":"Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams","title":"Installation"},{"location":"happy/adams/#starting-the-application","text":"Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh","title":"Starting the application"},{"location":"happy/adams/#available-flows","text":"Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool)","title":"Available flows"},{"location":"happy/adams/#tutorials","text":"Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation","title":"Tutorials"},{"location":"happy/adams/#preview-browser","text":"The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the happy-envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"Preview browser"},{"location":"happy/docker/","text":"To make it easier to use the libraries (and avoid any hassle with installing libraries and required dependencies), we also make them available as Docker images: happy-tools happy-tools-keras If you are not familiar with Docker, please feel free to have a look at the Docker for Data Scientists introduction.","title":"Docker"},{"location":"happy/installation/","text":"Prerequisites # Linux (Ubuntu/Debian) Docker ( sudo apt install docker.io ) redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Windows/WSL2 Ubuntu 22.04.x from the Microsoft store Docker redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Installation # Go to your home directory: cd ~ Download the happy-setup.sh script and make it executable: wget -O happy-setup.sh https://raw.githubusercontent.com/wairas/happy-scripts/main/happy-setup.sh chmod a+x happy-setup.sh Execute the script: ./happy-setup.sh Minimal installation items to execute: Prepare system (ensures that all required system libraries are present) Install Happy Tools Install SAM-HQ (or if you prefer, Install SAM ) Notes: SAM and SAM-HQ can be installed in parallel, but only one of them can actively running, as they both use the same redis channels for communication (that way they are interchangeable). ADAMS can be used for annotating objects in your scanned images.","title":"Installation"},{"location":"happy/installation/#prerequisites","text":"Linux (Ubuntu/Debian) Docker ( sudo apt install docker.io ) redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Windows/WSL2 Ubuntu 22.04.x from the Microsoft store Docker redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget )","title":"Prerequisites"},{"location":"happy/installation/#installation","text":"Go to your home directory: cd ~ Download the happy-setup.sh script and make it executable: wget -O happy-setup.sh https://raw.githubusercontent.com/wairas/happy-scripts/main/happy-setup.sh chmod a+x happy-setup.sh Execute the script: ./happy-setup.sh Minimal installation items to execute: Prepare system (ensures that all required system libraries are present) Install Happy Tools Install SAM-HQ (or if you prefer, Install SAM ) Notes: SAM and SAM-HQ can be installed in parallel, but only one of them can actively running, as they both use the same redis channels for communication (that way they are interchangeable). ADAMS can be used for annotating objects in your scanned images.","title":"Installation"},{"location":"happy/sam-hq/","text":"Segment Anything in High Quality (SAM-HQ) works like SAM and consists of pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh SAM scripts # In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the happy-envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam-hq directory, execute the start.sh script. Stopping # SAM # In the sam-hq directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM-HQ"},{"location":"happy/sam-hq/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam-hq/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam-hq/#windows","text":"WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam-hq/#directories","text":"sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models","title":"Directories"},{"location":"happy/sam-hq/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth","title":"Pretrained models"},{"location":"happy/sam-hq/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam-scripts","text":"In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the happy-envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam-hq/#starting","text":"","title":"Starting"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam","text":"In the sam-hq directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam-hq/#stopping","text":"","title":"Stopping"},{"location":"happy/sam-hq/#sam_1","text":"In the sam-hq directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/","text":"Facebook's Segment Anything (SAM) are pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh SAM scripts # In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam directory, execute the start.sh script. Stopping # SAM # In the sam directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM"},{"location":"happy/sam/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam/#windows","text":"WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam/#directories","text":"sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models","title":"Directories"},{"location":"happy/sam/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth","title":"Pretrained models"},{"location":"happy/sam/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam-scripts","text":"In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam/#starting","text":"","title":"Starting"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam","text":"In the sam directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam/#stopping","text":"","title":"Stopping"},{"location":"happy/sam/#sam_1","text":"In the sam directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/wsl2/","text":"Though all the tools are working under the Windows Subsystem for Linux , Docker and graphical applications only work as long as you are using version 2 of WSL and your Windows 10 is at least build 19044 (no restrictions with Windows 11) . NB: The following commands are all to be issued from a Windows command-prompt, not a Linux shell. First, make sure that your WSL is up-to-date (and supports graphical applications): wsl --update Here is how to switch WSL to version 2 as default by running the following command: wsl --set-default-version 2 You can view all your current images and what WSL version they are using with the following command: wsl --list -v If you want to switch an existing image (e.g., Ubuntu-22.04 ) to version 2, then you can run the following command: wsl --set-version Ubuntu-22.04 2 Source: https://stackoverflow.com/a/73164601 Advanced configuration # For more information on how to configure WSL2, please see here .","title":"WSL2"},{"location":"happy/wsl2/#advanced-configuration","text":"For more information on how to configure WSL2, please see here .","title":"Advanced configuration"},{"location":"happy/happy_tools/","text":"happy-tools contains several command-line utilities: happy-generate-image-regions-objects - generates datasets as numpy cubes for deep learning happy-hdr-info - outputs information on ENVI HDR files happy-hsi2rbg - generates fake RGB PNG files from HSI images happy-mat-info - outputs Matlab struct information happy-ann2happy - converts OPEX JSON polygon, ENVI pixel annotations and PNG images into happy data structures happy-opex-labels - performs actions on OPEX JSON files that it located happy-plot-preproc - plots set of pixels using various preprocessors happy-process-data - applies a preprocessing pipeline to the data happy-raw-check - sanity checks on raw capture folders happy-scikit-regression-build - evaluates regression models on HAPPy data happy-scikit-segmentation-build - evaluates segmentation models on HAPPy data happy-scikit-unsupervised-build - evaluates cluster models on HAPPy data happy-splitter - for generating train/validation/test splits for HAPPy data happy-sub-images - for exporting annotated sub-samples into separate files Graphical user interface tools using the tkinter widgets have been moved to the happy-tools-tkinter library. You can find the documentation on these tools here: happy-tools-tkinter Command-line and graphical tools are available from the Python virtual environment that they were installed in. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin Overview of available plugins and their respective parameters is located here: https://github.com/wairas/happy-tools/blob/main/plugins/README.md","title":"Overview"},{"location":"happy/happy_tools/happy-ann2happy/","text":"Command-line # usage: happy-ann2happy [-h] -i DIR [DIR ...] [--regexp REGEXP] [-c {pixels,polygons,pixels_then_polygons,polygons_then_pixels}] [-r] [-o DIR] -f {flat,dir-tree,dir-tree-with-data} -l LABELS [-N] [-u UNLABELLED] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--white_ref_annotations FILE] [--black_ref_locator_for_white_ref LOCATOR] [--black_ref_method_for_white_ref METHOD] [--pattern_mask PATTERN] [--pattern_labels PATTERN] [--pattern_png PATTERN] [--pattern_opex PATTERN] [--pattern_envi PATTERN] [-I] [-n] [--resume_from DIR] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Turns annotations (PNG, OPEX JSON, ENVI pixel annotations) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX/ENVI files (default: None) --regexp REGEXP The regexp for matching the ENVI base files (name only), e.g., for selecting a subset. (default: None) -c {pixels,polygons,pixels_then_polygons,polygons_then_pixels}, --conversion {pixels,polygons,pixels_then_polygons,polygons_then_pixels} What annotations and in what order to apply (subsequent overlays can overwrite annotations). (default: pixels_then_polygons) -r, --recursive whether to look for OPEX/ENVI files recursively (default: False) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree,dir-tree-with-data}, --output_format {flat,dir-tree,dir-tree-with-data} Defines how to store the data in the output directory. (default: dir-tree-with-data) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) -N, --no_implicit_background whether to require explicit annotations for the background rather than assuming all un-annotated pixels are background (default: False) -u UNLABELLED, --unlabelled UNLABELLED The value to use for pixels that do not have an explicit annotation (label values start after this value) (default: 0) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual; requires: dir-tree-with-data (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size; requires: dir-tree-with- data (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual; requires: dir-tree- with-data (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size; requires: dir-tree-with- data (default: None) --white_ref_annotations FILE the OPEX JSON file with the annotated white reference if it cannot be determined automatically (default: None) --black_ref_locator_for_white_ref LOCATOR the reference locator scheme to use for locating black references that get applied to the white reference, eg rl-manual (default: None) --black_ref_method_for_white_ref METHOD the black reference method to use for applying black references to the white reference, eg br-same-size (default: None) --pattern_mask PATTERN the pattern to use for saving the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.hdr) --pattern_labels PATTERN the pattern to use for saving the label map for the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.json) --pattern_png PATTERN the pattern to use for saving the mask PNG file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.png) --pattern_opex PATTERN the pattern to use for saving the OPEX JSON annotation file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.json) --pattern_envi PATTERN the pattern to use for saving the ENVI mask annotation file, available placeholders: {SAMPLEID} (default: MASK_{SAMPLEID}.hdr) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit generating any data or creating directories (default: False) --resume_from DIR The directory to restart the processing with (all determined dirs preceding this one get skipped) (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-ann2happy"},{"location":"happy/happy_tools/happy-ann2happy/#command-line","text":"usage: happy-ann2happy [-h] -i DIR [DIR ...] [--regexp REGEXP] [-c {pixels,polygons,pixels_then_polygons,polygons_then_pixels}] [-r] [-o DIR] -f {flat,dir-tree,dir-tree-with-data} -l LABELS [-N] [-u UNLABELLED] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--white_ref_annotations FILE] [--black_ref_locator_for_white_ref LOCATOR] [--black_ref_method_for_white_ref METHOD] [--pattern_mask PATTERN] [--pattern_labels PATTERN] [--pattern_png PATTERN] [--pattern_opex PATTERN] [--pattern_envi PATTERN] [-I] [-n] [--resume_from DIR] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Turns annotations (PNG, OPEX JSON, ENVI pixel annotations) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX/ENVI files (default: None) --regexp REGEXP The regexp for matching the ENVI base files (name only), e.g., for selecting a subset. (default: None) -c {pixels,polygons,pixels_then_polygons,polygons_then_pixels}, --conversion {pixels,polygons,pixels_then_polygons,polygons_then_pixels} What annotations and in what order to apply (subsequent overlays can overwrite annotations). (default: pixels_then_polygons) -r, --recursive whether to look for OPEX/ENVI files recursively (default: False) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree,dir-tree-with-data}, --output_format {flat,dir-tree,dir-tree-with-data} Defines how to store the data in the output directory. (default: dir-tree-with-data) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) -N, --no_implicit_background whether to require explicit annotations for the background rather than assuming all un-annotated pixels are background (default: False) -u UNLABELLED, --unlabelled UNLABELLED The value to use for pixels that do not have an explicit annotation (label values start after this value) (default: 0) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual; requires: dir-tree-with-data (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size; requires: dir-tree-with- data (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual; requires: dir-tree- with-data (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size; requires: dir-tree-with- data (default: None) --white_ref_annotations FILE the OPEX JSON file with the annotated white reference if it cannot be determined automatically (default: None) --black_ref_locator_for_white_ref LOCATOR the reference locator scheme to use for locating black references that get applied to the white reference, eg rl-manual (default: None) --black_ref_method_for_white_ref METHOD the black reference method to use for applying black references to the white reference, eg br-same-size (default: None) --pattern_mask PATTERN the pattern to use for saving the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.hdr) --pattern_labels PATTERN the pattern to use for saving the label map for the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.json) --pattern_png PATTERN the pattern to use for saving the mask PNG file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.png) --pattern_opex PATTERN the pattern to use for saving the OPEX JSON annotation file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.json) --pattern_envi PATTERN the pattern to use for saving the ENVI mask annotation file, available placeholders: {SAMPLEID} (default: MASK_{SAMPLEID}.hdr) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit generating any data or creating directories (default: False) --resume_from DIR The directory to restart the processing with (all determined dirs preceding this one get skipped) (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-generate-image-regions-objects/","text":"Command-line # usage: happy-generate-image-regions-objects [-h] -i INPUT_DIR -o OUTPUT_DIR Generate datasets as numpy cubes, to be loaded into deep learning datasets. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Path to source folder containing HDR files (default: None) -o OUTPUT_DIR, --output_dir OUTPUT_DIR Path to output folder (default: None)","title":"happy-generate-image-regions-objects"},{"location":"happy/happy_tools/happy-generate-image-regions-objects/#command-line","text":"usage: happy-generate-image-regions-objects [-h] -i INPUT_DIR -o OUTPUT_DIR Generate datasets as numpy cubes, to be loaded into deep learning datasets. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Path to source folder containing HDR files (default: None) -o OUTPUT_DIR, --output_dir OUTPUT_DIR Path to output folder (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hdr-info/","text":"Command-line # usage: happy-hdr-info [-h] -i INPUT_FILE [-f] [-o OUTPUT_FILE] Load and print information about an HDR file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the HDR file (default: None) -f, --full Whether to output all fields (default: False) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to output file; prints to stdout if omitted (default: None)","title":"happy-hdr-info"},{"location":"happy/happy_tools/happy-hdr-info/#command-line","text":"usage: happy-hdr-info [-h] -i INPUT_FILE [-f] [-o OUTPUT_FILE] Load and print information about an HDR file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the HDR file (default: None) -f, --full Whether to output all fields (default: False) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to output file; prints to stdout if omitted (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hsi2rbg/","text":"Command-line # usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-hsi2rbg"},{"location":"happy/happy_tools/happy-hsi2rbg/#command-line","text":"usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-mat-info/","text":"Command-line # usage: happy-mat-info [-h] -i INPUT_FILE [-o OUTPUT_FILE] Load and display structs from a MATLAB file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the MATLAB file (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output file; outputs to stdout if omitted (default: None)","title":"happy-mat-info"},{"location":"happy/happy_tools/happy-mat-info/#command-line","text":"usage: happy-mat-info [-h] -i INPUT_FILE [-o OUTPUT_FILE] Load and display structs from a MATLAB file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the MATLAB file (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output file; outputs to stdout if omitted (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-opex-labels/","text":"Command-line # usage: happy-opex-labels [-h] -i INPUT [-r] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] {list-labels,update-labels,delete-labels} ... Performs actions on OPEX JSON files that it locates. positional arguments: {list-labels,update-labels,delete-labels} sub-command help list-labels Lists the labels in the located files update-labels Updates the labels using the specified label mapping delete-labels Deletes the specified labels optional arguments: -h, --help show this help message and exit -i INPUT, --input INPUT The dir with the OPEX JSON files (default: None) -r, --recursive Whether to search the directory recursively (default: False) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-opex-labels"},{"location":"happy/happy_tools/happy-opex-labels/#command-line","text":"usage: happy-opex-labels [-h] -i INPUT [-r] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] {list-labels,update-labels,delete-labels} ... Performs actions on OPEX JSON files that it locates. positional arguments: {list-labels,update-labels,delete-labels} sub-command help list-labels Lists the labels in the located files update-labels Updates the labels using the specified label mapping delete-labels Deletes the specified labels optional arguments: -h, --help show this help message and exit -i INPUT, --input INPUT The dir with the OPEX JSON files (default: None) -r, --recursive Whether to search the directory recursively (default: False) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-plot-preproc/","text":"Command-line # usage: happy-plot-preproc [-h] -i INPUT_DIR [-f FROM_INDEX] [-t TO_INDEX] [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Plot set of pixels with various pre-processing setups. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Folder containing HAPPy data files (default: None) -f FROM_INDEX, --from_index FROM_INDEX The first wavelength index to include (0-based) (default: 60) -t TO_INDEX, --to_index TO_INDEX The last wavelength index to include (0-based) (default: 189) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data separately; use \"multi-pp\" if you need to combine multiple steps. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: pass- through multi-pp -p 'derivative -w 15 -d 0 snv' derivative -w 15 -d 0 sni) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: ps-simple -n 100 -b) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-plot-preproc"},{"location":"happy/happy_tools/happy-plot-preproc/#command-line","text":"usage: happy-plot-preproc [-h] -i INPUT_DIR [-f FROM_INDEX] [-t TO_INDEX] [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Plot set of pixels with various pre-processing setups. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Folder containing HAPPy data files (default: None) -f FROM_INDEX, --from_index FROM_INDEX The first wavelength index to include (0-based) (default: 60) -t TO_INDEX, --to_index TO_INDEX The last wavelength index to include (0-based) (default: 189) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data separately; use \"multi-pp\" if you need to combine multiple steps. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: pass- through multi-pp -p 'derivative -w 15 -d 0 snv' derivative -w 15 -d 0 sni) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: ps-simple -n 100 -b) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-process-data/","text":"Command-line # usage: happy-process-data reader [preprocessor(s)] writer [-h|--help|--help-all|--help-plugin NAME] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Processes data using the specified pipeline. readers: envi-reader, happy-reader, matlab-reader preprocessors: crop, derivative, divide-annotation-avg, down-sample, extract-regions, multi-pp, pca, pad, pass-through, snv, sni, std-scaler, subtract-annotation-avg, subtract, wavelength-subset writers: envi-writer, happy-writer, image-writer, matlab-writer, png-writer optional arguments: -h, --help show this help message and exit --help-all show the help for all plugins and exit --help-plugin NAME show the help for plugin NAME and exit -i [INPUT [INPUT ...]], --input [INPUT [INPUT ...]] Optional path to the file(s) to process in batch mode; glob syntax is supported (default: None) -I [INPUT_LIST [INPUT_LIST ...]], --input_list [INPUT_LIST [INPUT_LIST ...]] Optional path to the text file(s) listing the files to process in batch mode (default: None) -e REGEXP, --exclude REGEXP Regular expression for excluding files from batch processing; gets applied to full file path -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN) Examples # HappyData directory to Matlab # The following converts all samples in HappyData format in the /some/where/happy directory to Matlab, storing them in /some/where/matlab : happy-process-data \\ -V INFO \\ happy-reader \\ -V INFO \\ -b /some/where/happy \\ matlab-writer \\ -V INFO \\ -b /some/where/matlab ENVI files to JPG (batch processing) # The following command finds all .hdr files recursively below the directory /some/where/envi , turns them into JPEG images and stores them in /some/where/jpg : happy-process-data \\ -V INFO \\ -i \"/some/where/envi/**/*.hdr\" \\ --exclude \".*DARKREF_.*\" \".*MASK_.*\" \\ envi-reader \\ -V INFO \\ -b . \\ --exclude \"DARKREF_.*\" \"MASK_.*\" \\ image-writer \\ -V INFO \\ -b /some/where/jpg \\ -R 95 \\ -G 152 \\ -B 111 \\ -o \"{BASEDIR}/{SAMPLEID}.jpg\" \\ --suppress_metadata","title":"happy-process-data"},{"location":"happy/happy_tools/happy-process-data/#command-line","text":"usage: happy-process-data reader [preprocessor(s)] writer [-h|--help|--help-all|--help-plugin NAME] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Processes data using the specified pipeline. readers: envi-reader, happy-reader, matlab-reader preprocessors: crop, derivative, divide-annotation-avg, down-sample, extract-regions, multi-pp, pca, pad, pass-through, snv, sni, std-scaler, subtract-annotation-avg, subtract, wavelength-subset writers: envi-writer, happy-writer, image-writer, matlab-writer, png-writer optional arguments: -h, --help show this help message and exit --help-all show the help for all plugins and exit --help-plugin NAME show the help for plugin NAME and exit -i [INPUT [INPUT ...]], --input [INPUT [INPUT ...]] Optional path to the file(s) to process in batch mode; glob syntax is supported (default: None) -I [INPUT_LIST [INPUT_LIST ...]], --input_list [INPUT_LIST [INPUT_LIST ...]] Optional path to the text file(s) listing the files to process in batch mode (default: None) -e REGEXP, --exclude REGEXP Regular expression for excluding files from batch processing; gets applied to full file path -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-process-data/#examples","text":"","title":"Examples"},{"location":"happy/happy_tools/happy-process-data/#happydata-directory-to-matlab","text":"The following converts all samples in HappyData format in the /some/where/happy directory to Matlab, storing them in /some/where/matlab : happy-process-data \\ -V INFO \\ happy-reader \\ -V INFO \\ -b /some/where/happy \\ matlab-writer \\ -V INFO \\ -b /some/where/matlab","title":"HappyData directory to Matlab"},{"location":"happy/happy_tools/happy-process-data/#envi-files-to-jpg-batch-processing","text":"The following command finds all .hdr files recursively below the directory /some/where/envi , turns them into JPEG images and stores them in /some/where/jpg : happy-process-data \\ -V INFO \\ -i \"/some/where/envi/**/*.hdr\" \\ --exclude \".*DARKREF_.*\" \".*MASK_.*\" \\ envi-reader \\ -V INFO \\ -b . \\ --exclude \"DARKREF_.*\" \"MASK_.*\" \\ image-writer \\ -V INFO \\ -b /some/where/jpg \\ -R 95 \\ -G 152 \\ -B 111 \\ -o \"{BASEDIR}/{SAMPLEID}.jpg\" \\ --suppress_metadata","title":"ENVI files to JPG (batch processing)"},{"location":"happy/happy_tools/happy-raw-check/","text":"Command-line # usage: happy-raw-check [-h] -i INPUT [INPUT ...] [-r] [-o OUTPUT] [-f {text,text-compact,csv,json}] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Performs sanity checks on raw capture folders. optional arguments: -h, --help show this help message and exit -i INPUT [INPUT ...], --input INPUT [INPUT ...] The dir(s) with the raw capture folders (default: None) -r, --recursive Whether to search the directory recursively (default: False) -o OUTPUT, --output OUTPUT The file to store the results in; uses stdout if omitted (default: None) -f {text,text-compact,csv,json}, --output_format {text,text-compact,csv,json} The format to use for the output (default: text) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-raw-check"},{"location":"happy/happy_tools/happy-raw-check/#command-line","text":"usage: happy-raw-check [-h] -i INPUT [INPUT ...] [-r] [-o OUTPUT] [-f {text,text-compact,csv,json}] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Performs sanity checks on raw capture folders. optional arguments: -h, --help show this help message and exit -i INPUT [INPUT ...], --input INPUT [INPUT ...] The dir(s) with the raw capture folders (default: None) -r, --recursive Whether to search the directory recursively (default: False) -o OUTPUT, --output OUTPUT The file to store the results in; uses stdout if omitted (default: None) -f {text,text-compact,csv,json}, --output_format {text,text-compact,csv,json} The format to use for the output (default: text) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-regression-build/","text":"Command-line # usage: happy-scikit-regression-build [-h] -d HAPPY_DATA_BASE_DIR [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m REGRESSION_METHOD] [-p REGRESSION_PARAMS] -t TARGET_VALUE -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate regression model on Happy Data using specified splits and pixel selector. optional arguments: -h, --help show this help message and exit -d HAPPY_DATA_BASE_DIR, --happy_data_base_dir HAPPY_DATA_BASE_DIR Directory containing the Happy Data files (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 pad -W 128 -H 128 -v 0) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: ps-simple -n 64) -m REGRESSION_METHOD, --regression_method REGRESSION_METHOD Regression method name (e.g., linearregression,ridge,l ars,plsregression,plsneighbourregression,lasso,elastic net,decisiontreeregressor,randomforestregressor,svr or full class name) (default: linearregression) -p REGRESSION_PARAMS, --regression_params REGRESSION_PARAMS JSON string containing regression parameters (default: {}) -t TARGET_VALUE, --target_value TARGET_VALUE Target value column name (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-scikit-regression-build"},{"location":"happy/happy_tools/happy-scikit-regression-build/#command-line","text":"usage: happy-scikit-regression-build [-h] -d HAPPY_DATA_BASE_DIR [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m REGRESSION_METHOD] [-p REGRESSION_PARAMS] -t TARGET_VALUE -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate regression model on Happy Data using specified splits and pixel selector. optional arguments: -h, --help show this help message and exit -d HAPPY_DATA_BASE_DIR, --happy_data_base_dir HAPPY_DATA_BASE_DIR Directory containing the Happy Data files (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 pad -W 128 -H 128 -v 0) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: ps-simple -n 64) -m REGRESSION_METHOD, --regression_method REGRESSION_METHOD Regression method name (e.g., linearregression,ridge,l ars,plsregression,plsneighbourregression,lasso,elastic net,decisiontreeregressor,randomforestregressor,svr or full class name) (default: linearregression) -p REGRESSION_PARAMS, --regression_params REGRESSION_PARAMS JSON string containing regression parameters (default: {}) -t TARGET_VALUE, --target_value TARGET_VALUE Target value column name (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-segmentation-build/","text":"Command-line # usage: happy-scikit-segmentation-build [-h] -d HAPPY_DATA_BASE_DIR [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m SEGMENTATION_METHOD] [-p SEGMENTATION_PARAMS] -t TARGET_VALUE -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate segmentation model on Happy Data using specified splits and pixel selector. optional arguments: -h, --help show this help message and exit -d HAPPY_DATA_BASE_DIR, --happy_data_base_dir HAPPY_DATA_BASE_DIR Directory containing the Happy Data files (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data (default: ) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. (default: ps-simple -n 32767) -m SEGMENTATION_METHOD, --segmentation_method SEGMENTATION_METHOD Segmentation method name (e.g., randomforestclassifier ,gradientboostingclassifier,adaboostclassifier,kneighb orsclassifier,decisiontreeclassifier,gaussiannb,logist icregression,mlpclassifier,svm,random_forest,knn,decis ion_tree,gradient_boosting,naive_bayes,logistic_regres sion,neural_network,adaboost,extra_trees or full class name) (default: svm) -p SEGMENTATION_PARAMS, --segmentation_params SEGMENTATION_PARAMS JSON string containing segmentation parameters (default: {}) -t TARGET_VALUE, --target_value TARGET_VALUE Target value column name (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-scikit-segmentation-build"},{"location":"happy/happy_tools/happy-scikit-segmentation-build/#command-line","text":"usage: happy-scikit-segmentation-build [-h] -d HAPPY_DATA_BASE_DIR [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m SEGMENTATION_METHOD] [-p SEGMENTATION_PARAMS] -t TARGET_VALUE -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate segmentation model on Happy Data using specified splits and pixel selector. optional arguments: -h, --help show this help message and exit -d HAPPY_DATA_BASE_DIR, --happy_data_base_dir HAPPY_DATA_BASE_DIR Directory containing the Happy Data files (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data (default: ) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. (default: ps-simple -n 32767) -m SEGMENTATION_METHOD, --segmentation_method SEGMENTATION_METHOD Segmentation method name (e.g., randomforestclassifier ,gradientboostingclassifier,adaboostclassifier,kneighb orsclassifier,decisiontreeclassifier,gaussiannb,logist icregression,mlpclassifier,svm,random_forest,knn,decis ion_tree,gradient_boosting,naive_bayes,logistic_regres sion,neural_network,adaboost,extra_trees or full class name) (default: svm) -p SEGMENTATION_PARAMS, --segmentation_params SEGMENTATION_PARAMS JSON string containing segmentation parameters (default: {}) -t TARGET_VALUE, --target_value TARGET_VALUE Target value column name (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-unsupervised-build/","text":"Command-line # usage: happy-scikit-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m CLUSTERER_METHOD] [-p CLUSTERER_PARAMS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate clustering on hyperspectral data using specified clusterer and pixel selector. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Directory containing the hyperspectral data (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pca -n 5 -p 20) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: ps-simple -n 32 -b) -m CLUSTERER_METHOD, --clusterer_method CLUSTERER_METHOD Clusterer name (e.g., kmeans,agglomerative,spectral,dbscan,meanshift) or full class name (default: kmeans) -p CLUSTERER_PARAMS, --clusterer_params CLUSTERER_PARAMS JSON string containing clusterer parameters (default: {}) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-scikit-unsupervised-build"},{"location":"happy/happy_tools/happy-scikit-unsupervised-build/#command-line","text":"usage: happy-scikit-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m CLUSTERER_METHOD] [-p CLUSTERER_PARAMS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate clustering on hyperspectral data using specified clusterer and pixel selector. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Directory containing the hyperspectral data (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pca -n 5 -p 20) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: ps-simple -n 32 -b) -m CLUSTERER_METHOD, --clusterer_method CLUSTERER_METHOD Clusterer name (e.g., kmeans,agglomerative,spectral,dbscan,meanshift) or full class name (default: kmeans) -p CLUSTERER_PARAMS, --clusterer_params CLUSTERER_PARAMS JSON string containing clusterer parameters (default: {}) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-splitter/","text":"Command-line # usage: happy-splitter [-h] -b BASE_FOLDER [-r NUM_REPEATS] [-f NUM_FOLDS] [-t TRAIN_PERCENT] [-v VALIDATION_PERCENT] [-R] [-H HOLDOUT_PERCENT] -o OUTPUT_FILE [-S SEED] Generate train/validation/test splits for Happy data. optional arguments: -h, --help show this help message and exit -b BASE_FOLDER, --base_folder BASE_FOLDER Path to the Happy base folder (default: None) -r NUM_REPEATS, --num_repeats NUM_REPEATS Number of repeats (default: 1) -f NUM_FOLDS, --num_folds NUM_FOLDS Number of folds (default: 1) -t TRAIN_PERCENT, --train_percent TRAIN_PERCENT Percentage of data in the training set (default: 70.0) -v VALIDATION_PERCENT, --validation_percent VALIDATION_PERCENT Percentage of data in the validation set (default: 10.0) -R, --use_regions Use regions in generating splits (default: False) -H HOLDOUT_PERCENT, --holdout_percent HOLDOUT_PERCENT Percentage of data to hold out as a holdout set (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output split file (default: output_split.json) -S SEED, --seed SEED The seed to use for reproducible results (default: None)","title":"happy-splitter"},{"location":"happy/happy_tools/happy-splitter/#command-line","text":"usage: happy-splitter [-h] -b BASE_FOLDER [-r NUM_REPEATS] [-f NUM_FOLDS] [-t TRAIN_PERCENT] [-v VALIDATION_PERCENT] [-R] [-H HOLDOUT_PERCENT] -o OUTPUT_FILE [-S SEED] Generate train/validation/test splits for Happy data. optional arguments: -h, --help show this help message and exit -b BASE_FOLDER, --base_folder BASE_FOLDER Path to the Happy base folder (default: None) -r NUM_REPEATS, --num_repeats NUM_REPEATS Number of repeats (default: 1) -f NUM_FOLDS, --num_folds NUM_FOLDS Number of folds (default: 1) -t TRAIN_PERCENT, --train_percent TRAIN_PERCENT Percentage of data in the training set (default: 70.0) -v VALIDATION_PERCENT, --validation_percent VALIDATION_PERCENT Percentage of data in the validation set (default: 10.0) -R, --use_regions Use regions in generating splits (default: False) -H HOLDOUT_PERCENT, --holdout_percent HOLDOUT_PERCENT Percentage of data to hold out as a holdout set (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output split file (default: output_split.json) -S SEED, --seed SEED The seed to use for reproducible results (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-sub-images/","text":"Command-line # usage: happy-sub-images [-h] -i DIR [DIR ...] [-e REGEXP] [-r] [-o DIR [DIR ...]] [-w CMDLINE [CMDLINE ...]] [-l LABELS] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--white_ref_annotations FILE] [--black_ref_locator_for_white_ref LOCATOR] [--black_ref_method_for_white_ref METHOD] [--preprocessing PIPELINE] [-n] [-R DIR] [-I FILE] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Exports sub-images from ENVI files annotated with OPEX JSON files. Used for extracting sub-samples. Multiple output/writer pairs can be specified to output in multiple formats in one go. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the files to generate sub-images from (default: None) -e REGEXP, --regexp REGEXP The regexp for matching the ENVI base files (name only), e.g., for selecting a subset. (default: None) -r, --recursive whether to look for files recursively (default: False) -o DIR [DIR ...], --output_dir DIR [DIR ...] The dir(s) to store the generated sub-images in. (default: None) -w CMDLINE [CMDLINE ...], --writer CMDLINE [CMDLINE ...] the writer(s) to use for saving the generated sub- images (default: happy-writer) -l LABELS, --labels LABELS The regexp for the labels to export. (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) --white_ref_annotations FILE the OPEX JSON file with the annotated white reference if it cannot be determined automatically (default: None) --black_ref_locator_for_white_ref LOCATOR the reference locator scheme to use for locating black references that get applied to the white reference, eg rl-manual (default: None) --black_ref_method_for_white_ref METHOD the black reference method to use for applying black references to the white reference, eg br-same-size (default: None) --preprocessing PIPELINE the preprocessors to apply to the scan (default: None) -n, --dry_run whether to omit generating any data or creating directories (default: False) -R DIR, --resume_from DIR The directory to restart the processing with (all determined dirs preceding this one get skipped) (default: None) -I FILE, --run_info FILE The JSON file to store some run information in. (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN) Examples # Below are some examples for converting raw scans into various output formats. CSV # The command below instructs the csv-writer to combine all regions for a sample into a single file by not having the {REGION} (or {REPEAT} ) placeholder in the output pattern. {BASEDIR} is the output directory supplied to happy-sub-images . Black reference files are automatically determined using the --black_ref_locator option and the {PATH}/DARKREF_{NAME}.hdr pattern. The br-col-avg black reference method computes the average per band and column (requires scan and reference to have same number of columns). Only annotations that match the expression [0-9]+ are being exported: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/csv/ \\ --writer \"csv-writer -o {BASEDIR}/{SAMPLEID}.csv\" ENVI # The command below exports the regions of all samples separately as plain ENVI files. Just like with the CSV example above, black references are automatically determined based on their file name: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/envi/ \\ --writer envi-writer HappyData # By converting the scans into the HappyData format, you can view them later in the Data Viewer and then also build models. The command below only processes samples that match the regular expression \"A_1_1.*\" . The white reference for this subset of scans is stored in a separate scan folder ( --white_ref_locator \"rl-fixed ... ) and has its own OPEX JSON file with a single whiteref annotation ( --white_ref_annotations \"/some... ) which gets applied after the black reference has been applied. Since the white ref is a separate scan, a black reference can be applied as well using the options --black_ref_locator_for_white_ref and --black_ref_method_for_white_ref : happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ --regexp \"A_1_1.*\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ --white_ref_locator \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --white_ref_method wr-annotation-avg \\ --white_ref_annotations \"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.json\" \\ --black_ref_locator_for_white_ref \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/DARKREF_A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --black_ref_method_for_white_ref br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/happy/ \\ --writer happy-writer Matlab # The same command, but this time generating Matlab files using the matlab-writer : happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ --regexp \"A_1_1.*\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ --white_ref_locator \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --white_ref_method wr-annotation-avg \\ --white_ref_annotations \"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.json\" \\ --black_ref_locator_for_white_ref \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/DARKREF_A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --black_ref_method_for_white_ref br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/matlab/ \\ --writer matlab-writer PNG # Images can be generated with the image-writer , with the extension defined in the output pattern determining the image type. The --suppress_metadata option of the image-writer suppresses the output of the JSON companion files containing information about the region: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/png/ \\ --writer \"image-writer --suppress_metadata -R 95 -G 152 -B 111 -o {BASEDIR}/{SAMPLEID}.{REGION}.png\" Multiple output formats # By supplying multiple output/writer pairs, multiple output formats can be generated with a single pass. The following command generates CSV, Matlab and PNG output: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o \\ /some/where/csv/ \\ /some/where/matlab/ \\ /some/where/png/ \\ --writer \\ \"csv-writer -o {BASEDIR}/{SAMPLEID}.csv\" \\ matlab-writer \\ \"image-writer --suppress_metadata -R 95 -G 152 -B 111 -o {BASEDIR}/{SAMPLEID}.{REGION}.png\"","title":"happy-sub-images"},{"location":"happy/happy_tools/happy-sub-images/#command-line","text":"usage: happy-sub-images [-h] -i DIR [DIR ...] [-e REGEXP] [-r] [-o DIR [DIR ...]] [-w CMDLINE [CMDLINE ...]] [-l LABELS] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--white_ref_annotations FILE] [--black_ref_locator_for_white_ref LOCATOR] [--black_ref_method_for_white_ref METHOD] [--preprocessing PIPELINE] [-n] [-R DIR] [-I FILE] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Exports sub-images from ENVI files annotated with OPEX JSON files. Used for extracting sub-samples. Multiple output/writer pairs can be specified to output in multiple formats in one go. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the files to generate sub-images from (default: None) -e REGEXP, --regexp REGEXP The regexp for matching the ENVI base files (name only), e.g., for selecting a subset. (default: None) -r, --recursive whether to look for files recursively (default: False) -o DIR [DIR ...], --output_dir DIR [DIR ...] The dir(s) to store the generated sub-images in. (default: None) -w CMDLINE [CMDLINE ...], --writer CMDLINE [CMDLINE ...] the writer(s) to use for saving the generated sub- images (default: happy-writer) -l LABELS, --labels LABELS The regexp for the labels to export. (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) --white_ref_annotations FILE the OPEX JSON file with the annotated white reference if it cannot be determined automatically (default: None) --black_ref_locator_for_white_ref LOCATOR the reference locator scheme to use for locating black references that get applied to the white reference, eg rl-manual (default: None) --black_ref_method_for_white_ref METHOD the black reference method to use for applying black references to the white reference, eg br-same-size (default: None) --preprocessing PIPELINE the preprocessors to apply to the scan (default: None) -n, --dry_run whether to omit generating any data or creating directories (default: False) -R DIR, --resume_from DIR The directory to restart the processing with (all determined dirs preceding this one get skipped) (default: None) -I FILE, --run_info FILE The JSON file to store some run information in. (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools/happy-sub-images/#examples","text":"Below are some examples for converting raw scans into various output formats.","title":"Examples"},{"location":"happy/happy_tools/happy-sub-images/#csv","text":"The command below instructs the csv-writer to combine all regions for a sample into a single file by not having the {REGION} (or {REPEAT} ) placeholder in the output pattern. {BASEDIR} is the output directory supplied to happy-sub-images . Black reference files are automatically determined using the --black_ref_locator option and the {PATH}/DARKREF_{NAME}.hdr pattern. The br-col-avg black reference method computes the average per band and column (requires scan and reference to have same number of columns). Only annotations that match the expression [0-9]+ are being exported: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/csv/ \\ --writer \"csv-writer -o {BASEDIR}/{SAMPLEID}.csv\"","title":"CSV"},{"location":"happy/happy_tools/happy-sub-images/#envi","text":"The command below exports the regions of all samples separately as plain ENVI files. Just like with the CSV example above, black references are automatically determined based on their file name: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/envi/ \\ --writer envi-writer","title":"ENVI"},{"location":"happy/happy_tools/happy-sub-images/#happydata","text":"By converting the scans into the HappyData format, you can view them later in the Data Viewer and then also build models. The command below only processes samples that match the regular expression \"A_1_1.*\" . The white reference for this subset of scans is stored in a separate scan folder ( --white_ref_locator \"rl-fixed ... ) and has its own OPEX JSON file with a single whiteref annotation ( --white_ref_annotations \"/some... ) which gets applied after the black reference has been applied. Since the white ref is a separate scan, a black reference can be applied as well using the options --black_ref_locator_for_white_ref and --black_ref_method_for_white_ref : happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ --regexp \"A_1_1.*\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ --white_ref_locator \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --white_ref_method wr-annotation-avg \\ --white_ref_annotations \"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.json\" \\ --black_ref_locator_for_white_ref \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/DARKREF_A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --black_ref_method_for_white_ref br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/happy/ \\ --writer happy-writer","title":"HappyData"},{"location":"happy/happy_tools/happy-sub-images/#matlab","text":"The same command, but this time generating Matlab files using the matlab-writer : happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ --regexp \"A_1_1.*\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ --white_ref_locator \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --white_ref_method wr-annotation-avg \\ --white_ref_annotations \"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/A_White_Ref_2024-08-19_22-11-04.json\" \\ --black_ref_locator_for_white_ref \"rl-fixed -f \\\"/some/where/raw/A_White_Ref_2024-08-19_22-11-04/capture/DARKREF_A_White_Ref_2024-08-19_22-11-04.hdr\\\"\" \\ --black_ref_method_for_white_ref br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/matlab/ \\ --writer matlab-writer","title":"Matlab"},{"location":"happy/happy_tools/happy-sub-images/#png","text":"Images can be generated with the image-writer , with the extension defined in the output pattern determining the image type. The --suppress_metadata option of the image-writer suppresses the output of the JSON companion files containing information about the region: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o /some/where/png/ \\ --writer \"image-writer --suppress_metadata -R 95 -G 152 -B 111 -o {BASEDIR}/{SAMPLEID}.{REGION}.png\"","title":"PNG"},{"location":"happy/happy_tools/happy-sub-images/#multiple-output-formats","text":"By supplying multiple output/writer pairs, multiple output formats can be generated with a single pass. The following command generates CSV, Matlab and PNG output: happy-sub-images \\ -V INFO \\ -i \"/some/where/raw/\" \\ -r \\ --black_ref_locator \"rl-file-pattern -p \\\"{PATH}/DARKREF_{NAME}.hdr\\\"\" \\ --black_ref_method br-col-avg \\ -l \"[0-9]+\" \\ -o \\ /some/where/csv/ \\ /some/where/matlab/ \\ /some/where/png/ \\ --writer \\ \"csv-writer -o {BASEDIR}/{SAMPLEID}.csv\" \\ matlab-writer \\ \"image-writer --suppress_metadata -R 95 -G 152 -B 111 -o {BASEDIR}/{SAMPLEID}.{REGION}.png\"","title":"Multiple output formats"},{"location":"happy/happy_tools_keras/","text":"Additional tools utilizing the Keras deep learning library, available through happy-tools-keras : happy-keras-pixel-regression-build - evaluate a Keras-based pixel regression model happy-keras-segmentation-build - builds a Keras-based pixel segmentation model happy-keras-unsupervised-build - builds a Keras-based pixel segmentation model These tools are available from the Python virtual environment that they were installed. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin","title":"Overview"},{"location":"happy/happy_tools_keras/happy-keras-pixel-regression-build/","text":"Command-line # usage: happy-keras-pixel-regression-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate a Keras-based pixel regression model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: crop -W 320 -H 648 wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 320 -H 648 -v 0 down- sample) -t TARGET, --target TARGET Name of the target variable (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-keras-pixel-regression-build"},{"location":"happy/happy_tools_keras/happy-keras-pixel-regression-build/#command-line","text":"usage: happy-keras-pixel-regression-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Evaluate a Keras-based pixel regression model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: crop -W 320 -H 648 wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 320 -H 648 -v 0 down- sample) -t TARGET, --target TARGET Name of the target variable (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools_keras/happy-keras-segmentation-build/","text":"Command-line # usage: happy-keras-segmentation-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Build a Keras-based pixel segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-keras-segmentation-build"},{"location":"happy/happy_tools_keras/happy-keras-segmentation-build/#command-line","text":"usage: happy-keras-segmentation-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Build a Keras-based pixel segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools_keras/happy-keras-unsupervised-build/","text":"Command-line # usage: happy-keras-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET [-n NUM_CLUSTERS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Build a Keras-based unsupervised segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -n NUM_CLUSTERS, --num_clusters NUM_CLUSTERS The number of clusters to use (default: 4) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-keras-unsupervised-build"},{"location":"happy/happy_tools_keras/happy-keras-unsupervised-build/#command-line","text":"usage: happy-keras-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET [-n NUM_CLUSTERS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Build a Keras-based unsupervised segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -n NUM_CLUSTERS, --num_clusters NUM_CLUSTERS The number of clusters to use (default: 4) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools_tkinter/","text":"happy-tools-tkinter is an addon module that provides graphical user interfaces: happy-data-viewer - for viewing HAPPy data folders happy-envi-viewer - for viewing ENVI HSI images happy-raw-checker - sanity checks on raw capture folders","title":"Overview"},{"location":"happy/happy_tools_tkinter/happy-data-viewer/","text":"Used for viewing data that has been converted into a HAPPy folder structure. Menu # File Open dir: for opening HAPPy directories Export image: for exporting the currently displayed image Close: closes the application View Normalization: allows entering the command-line of another normalization plugin to change how the fake RGB is being generated. Zoom: allows the setting of the zoom level Window New window: starts a new Viewer process, using the same options that the current one was started with Half width: resizes the window to half of the screen width, useful when comparing two scans horizontally Half height: resizes the window to half of the screen height, useful when comparing two scans vertically Help: These menu items open browser windows with relevant pages. Screenshots # Here is an example of a directory containing several samples: When available, a mask can be overlaid with the appropriate opacity : Command-line # usage: happy-data-viewer [-h] [--base_folder BASE_FOLDER] [--sample SAMPLE] [--region REGION] [-r INT] [-g INT] [-b INT] [-o INT] [--listbox_selectbackground LISTBOX_SELECTBACKGROUND] [--listbox_selectforeground LISTBOX_SELECTFOREGROUND] [--normalization PLUGIN] [--zoom PERCENT] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Viewer for HAPPy data folder structures. optional arguments: -h, --help show this help message and exit --base_folder BASE_FOLDER Base folder to display (default: None) --sample SAMPLE The sample to load (default: None) --region REGION The region to load (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) -o INT, --opacity INT the opacity to use (0-100) (default: None) --listbox_selectbackground LISTBOX_SELECTBACKGROUND The background color to use for selected items in listboxes (default: #4a6984) --listbox_selectforeground LISTBOX_SELECTFOREGROUND The foreground color to use for selected items in listboxes (default: #ffffff) --normalization PLUGIN the normalization plugin and its options to use (default: norm-simple) --zoom PERCENT the initial zoom to use (%) or -1 for automatic fit (default: -1) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-data-viewer"},{"location":"happy/happy_tools_tkinter/happy-data-viewer/#menu","text":"File Open dir: for opening HAPPy directories Export image: for exporting the currently displayed image Close: closes the application View Normalization: allows entering the command-line of another normalization plugin to change how the fake RGB is being generated. Zoom: allows the setting of the zoom level Window New window: starts a new Viewer process, using the same options that the current one was started with Half width: resizes the window to half of the screen width, useful when comparing two scans horizontally Half height: resizes the window to half of the screen height, useful when comparing two scans vertically Help: These menu items open browser windows with relevant pages.","title":"Menu"},{"location":"happy/happy_tools_tkinter/happy-data-viewer/#screenshots","text":"Here is an example of a directory containing several samples: When available, a mask can be overlaid with the appropriate opacity :","title":"Screenshots"},{"location":"happy/happy_tools_tkinter/happy-data-viewer/#command-line","text":"usage: happy-data-viewer [-h] [--base_folder BASE_FOLDER] [--sample SAMPLE] [--region REGION] [-r INT] [-g INT] [-b INT] [-o INT] [--listbox_selectbackground LISTBOX_SELECTBACKGROUND] [--listbox_selectforeground LISTBOX_SELECTFOREGROUND] [--normalization PLUGIN] [--zoom PERCENT] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] Viewer for HAPPy data folder structures. optional arguments: -h, --help show this help message and exit --base_folder BASE_FOLDER Base folder to display (default: None) --sample SAMPLE The sample to load (default: None) --region REGION The region to load (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) -o INT, --opacity INT the opacity to use (0-100) (default: None) --listbox_selectbackground LISTBOX_SELECTBACKGROUND The background color to use for selected items in listboxes (default: #4a6984) --listbox_selectforeground LISTBOX_SELECTFOREGROUND The foreground color to use for selected items in listboxes (default: #ffffff) --normalization PLUGIN the normalization plugin and its options to use (default: norm-simple) --zoom PERCENT the initial zoom to use (%) or -1 for automatic fit (default: -1) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/","text":"Menu # In the sections below you can find short explanations of the various menu items. File # Clear all: removes all loaded data and annotations Open scan: opens an ENVI scan to view in the Image tab Import Pixel annotations: imports pixels annotations from an ENVI file Polygon annotations: imports polygon annotations from an OPEX JSON file Clear black reference: removes any black reference scan/annotations Open black reference: opens an ENVI scan to use as black reference (when using rl-manual ) Import black ref annotations: imports polygon annotations from an OPEX JSON file Clear white reference: removes any white reference scan/annotations Open white reference: opens an ENVI scan to use as white reference (when using rl-manual ) Import white ref annotations: imports polygon annotations from an OPEX JSON file Export Image: exports the current as image (PNG/JPG) using the RGB slider settings Pixel annotations: exports the current pixel annotations as ENVI file Polygon annotations: exports the current polygon annotations as OPEX JSON file Sub-images: allows exporting parts of the scan in various formats (as determined by bounding boxes derived from polygon annotations); useful when multiple samples are present in one image Export options Export to scan dir: uses the scan directory as initial directory when exporting annotations or images Overlay annotations: overlays the annotations onto the exported image Keep aspect ratio: keeps the aspect ratio of the scan when exporting the image Enforce MASK_ prefix: prefixes pixel annotations with MASK_ to ensure consistency Session: Open: load a session JSON file Save: save the current session data to a JSON file Close: closes the viewer application Edit # Undo: performs an undo of markers/annotations/meta-data Redo: restores the markers/annotations/meta-data Mode # Polygons: switches to polygon annotation mode Pixels: switches to pixel-wise annotation mode Markers # Clear: clears all markers Size: for setting the size of the marker circles Color: the color to use for the marker circles Min object size: the minimum object size that objects detected by SAM must have (applied to width and height) Run SAM: uses the current marker(s) as guidance point(s) for SAM to obtain outline detections Polygons # Clear: removes all polygon annotations Color: sets the color for the polygon overlays Modify: for deleting multiple polygons or changing multiple labels in one go Add polygon: creates a polygon from the markers, if there are at least three Add rectangle: creates a rectangular polygon from two markers (top-left and bottom-right) Pixels # Clear: removes all pixel annotations Brush shape: square or round shape Brush color: whether the brush cursor is depicted in black or white Brush size: lets the user choose the size of the brush Change alpha: the alpha value to use for the annotation overlay (0: transparent, 255: oaque) Select label: the label index determines the pixel value (0: background, >0: the label indices) Label key: generates a quick overview of the colors for the specified labels Meta-data # The meta-data managed with this menu will be exported with the polygon annotations. Clear: removes all meta-data Set: allows the user to enter a key-value pair Remove: allows the user to remove key-value pairs one at a time View: displays the currently set meta-data in a dialog View # Options Show pixel annotations: when checked any pixel annotations will be overlaid Show polygon annotations: when checked any polygon annotations will be overlaid Display spectra (raw): displays the spectra associated with the currently selected markers Display spectra (processed): displays the spectra of the processed data (after black/white ref and preprocessing has been applied) Zoom: allows various zoom levels (including custom level) or best fit Window # New window: starts a new Viewer process, using the same options that the current one was started with Half width: resizes the window to half of the screen width, useful when comparing two scans horizontally Half height: resizes the window to half of the screen height, useful when comparing two scans vertically Help # The menu items will open browser windows to web pages with relevant information. Overview # Image tab # Via File -> Open scan... , you can load an ENVI file representing a sample scan for viewing. Depending on your locators for the black/white references, you may need to load the black/white reference ENVI files manually. Some black/white reference methods rely on From that menu, you can also open black and white reference ENVI files that get applied to the scan, according to the : At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyperspectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file. Info tab # On the Info tab, you can see what files are currently loaded and what dimensions these files have: Options tab # On the Options tab, you can change various settings: General Auto-detect channels: uses any suggested channels from the meta-data of a scan Keep aspect ratio: whether to maintain width/height ratio of the scan or let the image fill the available canvas space Check scan dimensions: when checked and the dimensions of a subsequently loaded scan differs, a warning dialog will be displayed Auto-load annotations: when checked any available pixel or polygon annotations for scan, black and white reference get loaded automatically Predefined labels: comma-separated list of labels to use in the annotation process SAM connection parameters black reference location and method plugin white reference location and method plugin Preprocessing plugins Normalization plugin (gets applied before creating RGB image) Annotations # The Envi Viewer supports two types of annotations that can be exported: pixel-level: for disjointed/scattered regions of interest, e.g., ragged outlines of plants polygon-based: for well-defined shapes, like white references or shards of materials What type of annotations is active is managed via the Mode menu, where you can choose between Pixels and Polygons . Markers # Regardless of annotation mode, markers can be set on the scan image, as these markers are used as points of interest for displaying raw/processed spectra. left-clicking on the image sets a marker point. left-clicking while holding the CTRL key removes any marker points. Pixels # Ensure that you have switched to Pixels mode. Annotating pixels is done with these mouse actions: SHIFT+left-click: paints the pixels at the current location using the current brush and label; you can also drag the mouse while holding the keys to keep painting. CTRL+SHIFT+left-click: acts as eraser , removing any differently colored pixels at the current location SHIFT+right-click: performs flood fill at the current location, e.g., for filling in the center of an object after tracing the outline Before you start annotating, you should consider: Whether the cursor is better to see in black or white ( Pixels > Brush color ). Whether the cursor shape should be round or square ( Pixels > Brush shape ). What size the cursor should be ( Pixels > Brush size ; use an odd number as size to have a center from which to paint). Filling out centers of objects tends to be easier with a larger brush, before filling in the edges with a smaller one. Select the correct label for your annotations ( Pixels > Select label ). Using Pixels > Label key you can view a chart with the labels associated with their colors. Below is an annotation in progress: Once you are finished with the annotations, you can export them as ENVI file via: File > Export > Pixel annotations Polygons # Ensure that you have switched to Polygons mode. Polygons get created from markers as follows: Add markers in clockwise order where the vertices of the polygon should be, e.g., for outlining the white reference in the scan below: With at least three markers present, press CTRL+P or choose Add polygon from the Polygons menu to turn the markers into a polygon: You can assign a label to a polygon by left-clicking on the polygon while holding the SHIFT key. The labels whiteref and blackref are reserved keywords and used for white/black reference annotations. The labels available from the dropdown list are based on the predefined labels from the Options tab. Once a label has been selected and the dialog accepted, the label will be displayed in the center of the polygon: Apart from polygons, you can also create rectangles by using two markers for the top-left and bottom-right corners and then select Polygons > Add rectangle from the menu (or Ctrl+R). A useful feature when planning on exporting sub-images. Once you are finished with the annotations, you can export them as OPEX JSON file via: File > Export > Polygon annotations SAM # Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this: When in Pixels annotation mode, the polygons detected by SAM will be drawn as pixels: Sub-images export # When imaging multiple samples within a single scan, you can use the Sub-images export facility to separate out the sub-samples. The export requires the sub-samples to be annotated. Therefore, you need to: annotate each sub-sample with a rectangle: mark top-left and bottom-right corner and then select Add rectangle from the Polygons menu (or use CTRL+R shortcut) use the ID of the sub-sample as the label for the rectangle (SHIFT+left-click) Once all sub-samples have been annotate, export the polygon annotations and select File > Export > Sub-images from the menu: Select the directory to export the sub-samples to The Label regexp is a regular expression for identifying the labels in your annotations that represent the sub-samples. If you have no other annotations present, you can simply leave it empty. If that is not the case and your sub-samples are label with integers, you can use [0-9]+ . If you want to export the raw data, i.e., the original scan data, then tick the Use raw data checkbox. Otherwise, any changes from applying black/white reference and preprocessors will get output. Specify the writer and its parameters that you want to use for the export. See section Writer examples below the screenshot for some examples. The following screenshot exports the spectral data from the annotated sub-samples as CSV: Writer examples # Note: the {BASEDIR} placeholder used in -o parameters will get replaced by the selected output directory. Happy data: happy-writer ENVI: envi-writer PNG: png-writer JPG: image-writer -o {BASEDIR}/{SAMPLEID}.{REGIONID}.jpg CSV: csv-writer -o {BASEDIR}/{SAMPLEID}.{REGIONID}.csv CSV (combined sub-samples): csv-writer -o {BASEDIR}/{SAMPLEID}.csv Command-line # Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: happy-envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--no_autodetect_channels] [--keep_aspectratio] [--no_keep_aspectratio] [--check_scan_dimensions] [--no_check_scan_dimensions] [--auto_load_annotations] [--no_auto_load_annotations] [--export_to_scan_dir] [--annotation_color HEXCOLOR] [--predefined_labels LIST] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--no_redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--black_ref_locator_for_white_ref LOCATOR] [--black_ref_method_for_white_ref METHOD] [--preprocessing PIPELINE] [--log_timestamp_format FORMAT] [--zoom PERCENT] [--normalization PLUGIN] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] ENVI Hyperspectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --no_autodetect_channels whether to turn off auto-detection of channels from meta-data (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --no_keep_aspectratio whether to not keep the aspect ratio (default: None) --check_scan_dimensions whether to compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --no_check_scan_dimensions whether to not compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --auto_load_annotations whether to automatically load any annotations when loading a scan, black or white ref (default: None) --no_auto_load_annotations whether to not automatically load any annotations when loading a scan, black or white ref (default: None) --export_to_scan_dir whether to export images to the scan directory rather than the last one used (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --predefined_labels LIST the comma-separated list of labels to use (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --no_redis_connect whether to not immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) --black_ref_locator_for_white_ref LOCATOR the reference locator scheme to use for locating black references to apply to the white reference scans, eg rl-manual (default: None) --black_ref_method_for_white_ref METHOD the black reference method to use for applying the black reference to the white reference scans, eg br- same-size (default: None) --preprocessing PIPELINE the preprocessors to apply to the scan (default: None) --log_timestamp_format FORMAT the format string for the logging timestamp, see: http s://docs.python.org/3/library/datetime.html#strftime- and-strptime-format-codes (default: [%H:%M:%S.%f]) --zoom PERCENT the initial zoom to use (%) or -1 for automatic fit (default: -1) --normalization PLUGIN the normalization plugin and its options to use (default: norm-simple) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"happy-envi-viewer"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#menu","text":"In the sections below you can find short explanations of the various menu items.","title":"Menu"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#file","text":"Clear all: removes all loaded data and annotations Open scan: opens an ENVI scan to view in the Image tab Import Pixel annotations: imports pixels annotations from an ENVI file Polygon annotations: imports polygon annotations from an OPEX JSON file Clear black reference: removes any black reference scan/annotations Open black reference: opens an ENVI scan to use as black reference (when using rl-manual ) Import black ref annotations: imports polygon annotations from an OPEX JSON file Clear white reference: removes any white reference scan/annotations Open white reference: opens an ENVI scan to use as white reference (when using rl-manual ) Import white ref annotations: imports polygon annotations from an OPEX JSON file Export Image: exports the current as image (PNG/JPG) using the RGB slider settings Pixel annotations: exports the current pixel annotations as ENVI file Polygon annotations: exports the current polygon annotations as OPEX JSON file Sub-images: allows exporting parts of the scan in various formats (as determined by bounding boxes derived from polygon annotations); useful when multiple samples are present in one image Export options Export to scan dir: uses the scan directory as initial directory when exporting annotations or images Overlay annotations: overlays the annotations onto the exported image Keep aspect ratio: keeps the aspect ratio of the scan when exporting the image Enforce MASK_ prefix: prefixes pixel annotations with MASK_ to ensure consistency Session: Open: load a session JSON file Save: save the current session data to a JSON file Close: closes the viewer application","title":"File"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#edit","text":"Undo: performs an undo of markers/annotations/meta-data Redo: restores the markers/annotations/meta-data","title":"Edit"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#mode","text":"Polygons: switches to polygon annotation mode Pixels: switches to pixel-wise annotation mode","title":"Mode"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#markers","text":"Clear: clears all markers Size: for setting the size of the marker circles Color: the color to use for the marker circles Min object size: the minimum object size that objects detected by SAM must have (applied to width and height) Run SAM: uses the current marker(s) as guidance point(s) for SAM to obtain outline detections","title":"Markers"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#polygons","text":"Clear: removes all polygon annotations Color: sets the color for the polygon overlays Modify: for deleting multiple polygons or changing multiple labels in one go Add polygon: creates a polygon from the markers, if there are at least three Add rectangle: creates a rectangular polygon from two markers (top-left and bottom-right)","title":"Polygons"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#pixels","text":"Clear: removes all pixel annotations Brush shape: square or round shape Brush color: whether the brush cursor is depicted in black or white Brush size: lets the user choose the size of the brush Change alpha: the alpha value to use for the annotation overlay (0: transparent, 255: oaque) Select label: the label index determines the pixel value (0: background, >0: the label indices) Label key: generates a quick overview of the colors for the specified labels","title":"Pixels"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#meta-data","text":"The meta-data managed with this menu will be exported with the polygon annotations. Clear: removes all meta-data Set: allows the user to enter a key-value pair Remove: allows the user to remove key-value pairs one at a time View: displays the currently set meta-data in a dialog","title":"Meta-data"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#view","text":"Options Show pixel annotations: when checked any pixel annotations will be overlaid Show polygon annotations: when checked any polygon annotations will be overlaid Display spectra (raw): displays the spectra associated with the currently selected markers Display spectra (processed): displays the spectra of the processed data (after black/white ref and preprocessing has been applied) Zoom: allows various zoom levels (including custom level) or best fit","title":"View"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#window","text":"New window: starts a new Viewer process, using the same options that the current one was started with Half width: resizes the window to half of the screen width, useful when comparing two scans horizontally Half height: resizes the window to half of the screen height, useful when comparing two scans vertically","title":"Window"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#help","text":"The menu items will open browser windows to web pages with relevant information.","title":"Help"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#overview","text":"","title":"Overview"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#image-tab","text":"Via File -> Open scan... , you can load an ENVI file representing a sample scan for viewing. Depending on your locators for the black/white references, you may need to load the black/white reference ENVI files manually. Some black/white reference methods rely on From that menu, you can also open black and white reference ENVI files that get applied to the scan, according to the : At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyperspectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file.","title":"Image tab"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#info-tab","text":"On the Info tab, you can see what files are currently loaded and what dimensions these files have:","title":"Info tab"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#options-tab","text":"On the Options tab, you can change various settings: General Auto-detect channels: uses any suggested channels from the meta-data of a scan Keep aspect ratio: whether to maintain width/height ratio of the scan or let the image fill the available canvas space Check scan dimensions: when checked and the dimensions of a subsequently loaded scan differs, a warning dialog will be displayed Auto-load annotations: when checked any available pixel or polygon annotations for scan, black and white reference get loaded automatically Predefined labels: comma-separated list of labels to use in the annotation process SAM connection parameters black reference location and method plugin white reference location and method plugin Preprocessing plugins Normalization plugin (gets applied before creating RGB image)","title":"Options tab"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#annotations","text":"The Envi Viewer supports two types of annotations that can be exported: pixel-level: for disjointed/scattered regions of interest, e.g., ragged outlines of plants polygon-based: for well-defined shapes, like white references or shards of materials What type of annotations is active is managed via the Mode menu, where you can choose between Pixels and Polygons .","title":"Annotations"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#markers_1","text":"Regardless of annotation mode, markers can be set on the scan image, as these markers are used as points of interest for displaying raw/processed spectra. left-clicking on the image sets a marker point. left-clicking while holding the CTRL key removes any marker points.","title":"Markers"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#pixels_1","text":"Ensure that you have switched to Pixels mode. Annotating pixels is done with these mouse actions: SHIFT+left-click: paints the pixels at the current location using the current brush and label; you can also drag the mouse while holding the keys to keep painting. CTRL+SHIFT+left-click: acts as eraser , removing any differently colored pixels at the current location SHIFT+right-click: performs flood fill at the current location, e.g., for filling in the center of an object after tracing the outline Before you start annotating, you should consider: Whether the cursor is better to see in black or white ( Pixels > Brush color ). Whether the cursor shape should be round or square ( Pixels > Brush shape ). What size the cursor should be ( Pixels > Brush size ; use an odd number as size to have a center from which to paint). Filling out centers of objects tends to be easier with a larger brush, before filling in the edges with a smaller one. Select the correct label for your annotations ( Pixels > Select label ). Using Pixels > Label key you can view a chart with the labels associated with their colors. Below is an annotation in progress: Once you are finished with the annotations, you can export them as ENVI file via: File > Export > Pixel annotations","title":"Pixels"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#polygons_1","text":"Ensure that you have switched to Polygons mode. Polygons get created from markers as follows: Add markers in clockwise order where the vertices of the polygon should be, e.g., for outlining the white reference in the scan below: With at least three markers present, press CTRL+P or choose Add polygon from the Polygons menu to turn the markers into a polygon: You can assign a label to a polygon by left-clicking on the polygon while holding the SHIFT key. The labels whiteref and blackref are reserved keywords and used for white/black reference annotations. The labels available from the dropdown list are based on the predefined labels from the Options tab. Once a label has been selected and the dialog accepted, the label will be displayed in the center of the polygon: Apart from polygons, you can also create rectangles by using two markers for the top-left and bottom-right corners and then select Polygons > Add rectangle from the menu (or Ctrl+R). A useful feature when planning on exporting sub-images. Once you are finished with the annotations, you can export them as OPEX JSON file via: File > Export > Polygon annotations","title":"Polygons"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#sam","text":"Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this: When in Pixels annotation mode, the polygons detected by SAM will be drawn as pixels:","title":"SAM"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#sub-images-export","text":"When imaging multiple samples within a single scan, you can use the Sub-images export facility to separate out the sub-samples. The export requires the sub-samples to be annotated. Therefore, you need to: annotate each sub-sample with a rectangle: mark top-left and bottom-right corner and then select Add rectangle from the Polygons menu (or use CTRL+R shortcut) use the ID of the sub-sample as the label for the rectangle (SHIFT+left-click) Once all sub-samples have been annotate, export the polygon annotations and select File > Export > Sub-images from the menu: Select the directory to export the sub-samples to The Label regexp is a regular expression for identifying the labels in your annotations that represent the sub-samples. If you have no other annotations present, you can simply leave it empty. If that is not the case and your sub-samples are label with integers, you can use [0-9]+ . If you want to export the raw data, i.e., the original scan data, then tick the Use raw data checkbox. Otherwise, any changes from applying black/white reference and preprocessors will get output. Specify the writer and its parameters that you want to use for the export. See section Writer examples below the screenshot for some examples. The following screenshot exports the spectral data from the annotated sub-samples as CSV:","title":"Sub-images export"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#writer-examples","text":"Note: the {BASEDIR} placeholder used in -o parameters will get replaced by the selected output directory. Happy data: happy-writer ENVI: envi-writer PNG: png-writer JPG: image-writer -o {BASEDIR}/{SAMPLEID}.{REGIONID}.jpg CSV: csv-writer -o {BASEDIR}/{SAMPLEID}.{REGIONID}.csv CSV (combined sub-samples): csv-writer -o {BASEDIR}/{SAMPLEID}.csv","title":"Writer examples"},{"location":"happy/happy_tools_tkinter/happy-envi-viewer/#command-line","text":"Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: happy-envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--no_autodetect_channels] [--keep_aspectratio] [--no_keep_aspectratio] [--check_scan_dimensions] [--no_check_scan_dimensions] [--auto_load_annotations] [--no_auto_load_annotations] [--export_to_scan_dir] [--annotation_color HEXCOLOR] [--predefined_labels LIST] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--no_redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--black_ref_locator_for_white_ref LOCATOR] [--black_ref_method_for_white_ref METHOD] [--preprocessing PIPELINE] [--log_timestamp_format FORMAT] [--zoom PERCENT] [--normalization PLUGIN] [-V {DEBUG,INFO,WARNING,ERROR,CRITICAL}] ENVI Hyperspectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --no_autodetect_channels whether to turn off auto-detection of channels from meta-data (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --no_keep_aspectratio whether to not keep the aspect ratio (default: None) --check_scan_dimensions whether to compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --no_check_scan_dimensions whether to not compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --auto_load_annotations whether to automatically load any annotations when loading a scan, black or white ref (default: None) --no_auto_load_annotations whether to not automatically load any annotations when loading a scan, black or white ref (default: None) --export_to_scan_dir whether to export images to the scan directory rather than the last one used (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --predefined_labels LIST the comma-separated list of labels to use (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --no_redis_connect whether to not immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) --black_ref_locator_for_white_ref LOCATOR the reference locator scheme to use for locating black references to apply to the white reference scans, eg rl-manual (default: None) --black_ref_method_for_white_ref METHOD the black reference method to use for applying the black reference to the white reference scans, eg br- same-size (default: None) --preprocessing PIPELINE the preprocessors to apply to the scan (default: None) --log_timestamp_format FORMAT the format string for the logging timestamp, see: http s://docs.python.org/3/library/datetime.html#strftime- and-strptime-format-codes (default: [%H:%M:%S.%f]) --zoom PERCENT the initial zoom to use (%) or -1 for automatic fit (default: -1) --normalization PLUGIN the normalization plugin and its options to use (default: norm-simple) -V {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL} The logging level to use. (default: WARN)","title":"Command-line"},{"location":"happy/happy_tools_tkinter/happy-raw-checker/","text":"Main window # The Raw checker user interface performs some sanity checks on the capture folders containing the raw data. These checks are then displayed for the user, allowing them to check whether folders have been left out from annotating or whether incorrect label names were used. File menu: Select dir... - the directory to analyze (looks recursively for capture folders beneath this folder) Save output... - for saving an analysis to a file Edit menu: Copy - copies the currently displayed output to the clipboard View menu: Output format... - for selecting the type of output to generate Menu items from the Help menu open browser windows with relevant web pages. Command-line # usage: happy-raw-checker [-h] [-d RAW_DIR] [-f {text,text-compact,csv,json}] Raw data checker interface. For sanity checks of raw capture data. optional arguments: -h, --help show this help message and exit -d RAW_DIR, --raw_dir RAW_DIR The initial directory (default: None) -f {text,text-compact,csv,json}, --output_format {text,text-compact,csv,json} The output format to use in the text box. (default: text)","title":"happy-raw-checker"},{"location":"happy/happy_tools_tkinter/happy-raw-checker/#main-window","text":"The Raw checker user interface performs some sanity checks on the capture folders containing the raw data. These checks are then displayed for the user, allowing them to check whether folders have been left out from annotating or whether incorrect label names were used. File menu: Select dir... - the directory to analyze (looks recursively for capture folders beneath this folder) Save output... - for saving an analysis to a file Edit menu: Copy - copies the currently displayed output to the clipboard View menu: Output format... - for selecting the type of output to generate Menu items from the Help menu open browser windows with relevant web pages.","title":"Main window"},{"location":"happy/happy_tools_tkinter/happy-raw-checker/#command-line","text":"usage: happy-raw-checker [-h] [-d RAW_DIR] [-f {text,text-compact,csv,json}] Raw data checker interface. For sanity checks of raw capture data. optional arguments: -h, --help show this help message and exit -d RAW_DIR, --raw_dir RAW_DIR The initial directory (default: None) -f {text,text-compact,csv,json}, --output_format {text,text-compact,csv,json} The output format to use in the text box. (default: text)","title":"Command-line"}]}